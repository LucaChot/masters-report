\chapter*{Abstract}

Kubernetes is an existing widely-used open-source container orchestration system
that automates the deployment, scaling and management of containerized
applications. Efficient scheduling is a critical component of Kubernetes,
dictating an optimal allocation of pods across nodes to maximise a set of goals.
Existing Kubernetes schedulers can be classified according to the input data
they use to inform scheduling decisions: pod-description vs. telemetry. The
performance of pod-description based schedulers relies on accurate workload
estimations which is not always known beforehand. Meanwhile, telemetry-based
schedulers use live information to maximise resource utilisation of nodes and
prevent them from becoming overloaded. These telemetry-based schedulers
typically use utilisation metrics which can be overly sensitive and generate
false notifications of full capacity.

I decided to explore the application of the theory behind Pronto, a federated
asynchronous memory-limited algorithm proposed for task-scheduling across
large-scale networks of hundreds of workers, to scheduling within Kubernetes.
Each individual node updates their own local model based on the workload seen so
far (contention-based telemetry) and uses peak-prediction generate a rejection
signal which reflects the overall node responsiveness and whether it can accept
an incoming task. In addition, aggregating the local models builds a global view
of the system. However, as the paper assumes no communication latency and spikes
within a Kubernetes node's contention-based metric are not reflective of
over-contention, peak-prediction is no longer an accurate indicator.

As a result I propose a novel algorithm, Spazio\footnote{Spazio: space in
Italian}, with the same properties of Pronto while also allowing the existance
of communication latency. Nodes in Spazio update their local model with
capacity-based metrics and score themselves according to their predicted
capacity: how well a nodes current resource usage compliments recent measured
workload. Furthermore, Spazio also utilises contention-based metrics to more
accurately measure the resource capacity of a node.

To evaluate Spazio, I implement a prototype of Spazio in Kubernetes, evaluating
individual components to ensure correctness of behaviour. Finally, I compare
Spazio against the industry-standard \verb|kube-scheduler| (the default
scheduler of Kubernetes), evaluating Spazio's throughput and its
quality-of-service (QoS). From the evaluation, I show how Spazio can achieve
high workload-isolation, low latency while achieving comparable throughput
compared to Kubernetes.
