\chapter{Introduction (961)}
\label{firstcontentpage} % start page count here

% This is the introduction where you should introduce your work. In
% general the thing to aim for here is to describe a little bit of the
% context for your work -- why did you do it (motivation), what was the
% hoped-for outcome (aims) -- as well as trying to give a brief overview
% of what you actually did.
%
% It's often useful to bring forward some ``highlights'' into this
% chapter (e.g.\ some particularly compelling results, or a particularly
% interesting finding).
%
% It's also traditional to give an outline of the rest of the document,
% although without care this can appear formulaic and tedious. Your
% call.

% Kubernetes is a widely-used open-source container orchestration system that
% automates the deployment, scaling and management of containerized applications.
% It can run production workloads, having becoming the foundation for many cloud
% computing services like Google Cloud and AWS, while also being portable and
% extensible with a rapidly growing ecosystem of support and tools. Efficient
% scheduling is a critical component of Kubernetes, dictating how deployments
% are best allocated across nodes and ensuring no individual node is overloaded.
% Existing Kubernetes schedulers can be classified according to the input data
% they use to inform scheduling decisions: pod-description vs. telemetry. The
% performance of pod-description based schedulers relies on accurate workload
% estimations which is not always available. Meanwhile, telemetry-based schedulers
% use live information to maximise resource utilisation of nodes and prevent them
% from becoming overloaded. These telemetry-based schedulers typically use
% utilisation metrics which can be overly sensitive and generate false
% notifications of full capacity.
%

Kubernetes has established itself as the leading platform for container
orchestration, experiencing widespread adoption across diverse industries and
organizations of all scales \cite{kubernetes-adoption-statistics}. From
traditional stateless microservices that power modern web applications and APIs,
to complex stateful databases requiring persistent storage, Kubernetes provides
a robust and flexible foundation. It seamlessly orchestrates high-throughput
batch processing jobs, streamlines continuous integration and delivery (CI/CD)
pipelines, and has become the infrastructure backbone for cloud computing
services like Google Cloud and Azure \cite{google-gke, azure-aks}. Furthermore,
its versatility extends to supporting machine learning (ML) deployments
\cite{kubernetes-ai}, edge computing \cite{cloudraft}, and even serverless
functions \cite{knative, openwhisk}. This broad applicability solidifies
Kubernetes' position as a cornerstone of the cloud-native industry.

A core function of container orchestration is the scheduling of containers
across a cluster. Task scheduling is classified as an NP-hard problem and
consists of organising, prioritising and allocating tasks to a set of resources
in an optimal manner. Kubernetes performs online scheduling, a more challenging
subset of scheduling \cite{pruhs2004online} where schedulers receive tasks over
time, and must schedule the tasks without any knowledge of the future. Without
entirely knowing of all tasks to come, the scheduler can't guarantee optimal
schedules. Much research has been focused on finding efficient scheduling
algorithms that guarantee solutions as close to optimal as possible. The
standard Kubernetes approach assumes a set of pre-determined Node capacities and
resource requirements, translating the problem of efficient scheduling into one
of bin-packing \cite{}.

In the dynamic and resource-intensive landscape of containerized applications,
efficient online scheduling is paramount for both performance and
cost-effectiveness. Poor scheduling decisions can lead to significant
infrastructure waste, directly impacting an organisation's bottom line.
Misconfiguration in resource requests and limits are a primary cause of
under-utilised resources or idle nodes, and thus higher billing
\cite{cost-strategies,
bin-packing-and-cost-savings-in-kubernetes-clusters-on-aws}.

TODO: SHOULD I MOVE THE PARAGRAPH ABOVE TO THE TOP TO HAVE A MORE IMPACTFUL
START

The available Kubernetes schedulers can be divided based on the information they
use to inform their scheduling decisions: pod-description based schedulers,
which use pod annotations to declare resource requests, resource limits and QoS classes,
and telemetry-based schedulers, which periodically collected live information
from each node. As previously mentioned, pod-description based schedulers rely on
accurate resource usage estimations. Telemetry-based schedulers can circumvent this
problem, collecting information about the true resource usage of a Node to then
influence future scheduling decisions. However, to the best of my knowledge,
all these telemetric schedulers only aim to improve the estimations used by the
existing default \texttt{kube-scheduler}.

The concept of a Kubernetes scheduler operating without predefined pod resource
requests remains largely unexplored due to its inherent complexity. However,
this avenue warrants further investigation. We cannot always guarantee accurate
workload estimations beforehand, rendering traditional bin-packing approaches
"best-effort" at best. A purely telemetry-driven scheduler could potentially
overcome this limitation by adapting to true resource consumption, even for
unpredictable workloads.

PRONTO \cite{} offers a promising direction. This novel federated, asynchronous,
memory-limited algorithm schedules tasks across large-scale networks of hundreds
of workers. Each individual worker updates their local model based on the
workload seen so far which is then periodically aggregating the local models
builds a global view of the system. For a telemetric-only scheduler, maximizing
information aggregation is crucial. However, in very large clusters,
centralizing all this data quickly becomes a bottleneck. By distributing the
knowledge federation workload across the cluster, we can significantly reduce
the load on a central scheduler. Unfortunately, its assumptions of no
communication latency, and initial empirical findings when investigating
metrics indicated that a direct application would not be feasible.

Instead, I devised a new algorithm, CARICO (Italian for "load"), that is still
federated, asynchronous and memory-limited like PRONTO, but accounts for
communication latency. Nodes perform FSVD on capacity-based metrics, rather than
FPCA, modeling past resource usage to estimate future workload. Nodes can then
use their current resource usage and estimated workload to produce a measure of
capacity which can be used to score Nodes. Furthermore, when combined with
simple signal processing, this signal can be transformed to represent the number
of Pods a Node is willing to accept, allowing schedulers to handle communication
latencies by reserving portions of the signal with each scheduling decision.

In contrast, I developed CARICO (Italian for "load"), a novel algorithm that
maintains the federated, asynchronous, and memory-limited properties of PRONTO
while explicitly incorporating communication latency considerations. Nodes
perform Federated Singular Value Decomposition (FSVD)
on capacity-based metrics, rather than Federated Principal Component Analysis
(FPCA), to model historical resource utilisation and forecast future workload.
Based on their current resource usage and estimated workload, Nodes
compute a dynamic capacity metric which serves as a basis for Node scoring.
Moreover, through the application of simple signal processing, this capacity
signal can be transformed to quantify the number of Pods a Node is prepared to
accept, thereby allowing schedulers to account for communication latencies by
reserving portions of the signal with each scheduling decision.

For the evaluation, I implement CARICO within the Kubernetes ecosystem and run
it within a Kubernetes Cluster. This required extensive investigations into
various sources of telemetry, metrics and signal processing with the goal of
generating an an accurate and precise capacity signal. I evaluate CARICO's
overall performance when scheduling various workloads on a Kubernetes cluster,
comparing it to the industry-standard default \texttt{kube-scheduler}. I show
that while CARICO achieves comparable throughput, it significantl outperforms as
a QoS scheduler, demonstrating lower Pod Completion times and improved workload
isolation.

To sum up, this project makes the following contributions:
\begin{itemize}
\item Investigate the feasibility of applying PRONTO to Kubernetes scheduling
\item Propose a novel scoring algorithm, CARICO, with the same properties as
    PRONTO, while also accounting for possible communication latency.
    \begin{itemize}
        \item Novel application of FSVD to build a local model of recent
            resource usage. Adapted the standard SVD subspace merge operation to
            account for a lack of mean-centering.
        \item Present a novel signal that scores a Node's predicted capacity
            considering recent workloads experienced across the cluster.
        \item Apply simple signal processing techniques to transform the
            generated capacity signal into a signal that can be reserved.
    \end{itemize}
\item Implement a prototype of CARICO to evaluate the algorithm when applied to
    a real-world cluster and workloads.
    \begin{itemize}
        \item Investigate different metrics to use to build the local model,
            elucidating the problems of using metrics advertised by Virtual
            Machines (VMs). Explore different signal processing techniques to
            reduce noise generated by the container runtime.
        \item Ensure correctness of the generated signal and investigate different
            approaches to estimating the baseline signal value and the signal
            cost associated with a single Pod.
        \item Investigate the overall performance of CARICO under different
            workloads. I focus on throughput, QoS, and performance isolation.
    \end{itemize}
\end{itemize}

I Chapter 2, I go over existing Kubernetes schedulers, highlighting how only Pod
description-based schedulers or telemetry-based schedulers which aim to optimise
decisions of already running Pod description-based schedulers exist. I conduct a
brief investigation into the feasibility of applying Pronto to Kubernetes,
highlighting flawed assumptions and presenting empirical evidence. In Chapter 3,
I propose the novel algorithm, CARICO, using proofs and reasoning to explain the
generated capacity signal. In Chapter 4, I outline the structure of the CARICO
system in Kubernetes, exploring  numerous metrics, signal processing techniques
with the goal of generating an accurate and precise signal. In Chapter 5, I
evaluate the performance of CARICO under different workloads, collecting
numerous performance metrics and comparing it against the standard
\verb|kube-scheduler|. Lastly, I conclude my work and propose potential future
directions.

