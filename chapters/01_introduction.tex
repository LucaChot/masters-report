\pagenumbering{arabic}
\chapter{Introduction}
\label{firstcontentpage} % start page count here

% This is the introduction where you should introduce your work. In
% general the thing to aim for here is to describe a little bit of the
% context for your work -- why did you do it (motivation), what was the
% hoped-for outcome (aims) -- as well as trying to give a brief overview
% of what you actually did.
%
% It's often useful to bring forward some ``highlights'' into this
% chapter (e.g.\ some particularly compelling results, or a particularly
% interesting finding).
%
% It's also traditional to give an outline of the rest of the document,
% although without care this can appear formulaic and tedious. Your
% call.

% Kubernetes is a widely-used open-source container orchestration system that
% automates the deployment, scaling and management of containerized applications.
% It can run production workloads, having becoming the foundation for many cloud
% computing services like Google Cloud and AWS, while also being portable and
% extensible with a rapidly growing ecosystem of support and tools. Efficient
% scheduling is a critical component of Kubernetes, dictating how deployments
% are best allocated across nodes and ensuring no individual node is overloaded.
% Existing Kubernetes schedulers can be classified according to the input data
% they use to inform scheduling decisions: pod-description vs. telemetry. The
% performance of pod-description based schedulers relies on accurate workload
% estimations which is not always available. Meanwhile, telemetry-based schedulers
% use live information to maximise resource utilisation of nodes and prevent them
% from becoming overloaded. These telemetry-based schedulers typically use
% utilisation metrics which can be overly sensitive and generate false
% notifications of full capacity.
%

Datacenters serve as the foundational infrastructure for modern computing,
housing hundreds of thousands of machines that underpin diverse user
applications and data-parallel computations. The escalating demand for compute,
memory, storage, and network resources necessitates increasingly large and
complex datacenters. Datacenter scheduling is a critical task: it involves
allocating available resources to workloads to ensure performance objectives
are met with any small deviations resulting in potential revenue losses of
millions of dollars~\cite{barroso2019datacenter}. The difficulty of this problem is compounded by
ever-increasing input workload rates, dynamically changing workload
characteristics, and the heterogeneity of resources. Schedulers are expected to
deliver short user response times, high resource utilization, and high
scheduling throughput simultaneously, making it one of the most challenging
aspects of datacenter operation.

Numerous approaches have been explored to tackle task scheduling at a
datacenter-level. Multi-dimensional resource allocation attempt to reduce
scheduling to a bin packing problem, with many schedulers~\cite{verma2015large,
ungureanu2019kubernetes}
exploiting user-provided job resource requests to influence allocations.
However, the problem of inaccurate resource estimation has necessitated complex
techniques to better estimate resource requirements~\cite{delimitrou2014quasar, grandl2016altruistic, venkataraman2016ernest}. Another approach
has schedulers predict the availability of node resources. These schedulers
initially consisted of those that probed a portion of the datacenter
on-demand~\cite{ousterhout2013sparrow}, and those that performed offline machine learning on
telemetry from the entire datacenter~\cite{cortez2017resource}. While existing schedulers had to
trade responsiveness for a holistic model of the datacenter, \textsc{Pronto} was
proposed to achieve both. \textsc{Pronto} is a novel federated, asynchronous,
and memory-limited algorithm that exploits federated learning (FL) techniques to
distribute knowledge aggregation and processing across all the compute nodes in
the datacenter.

With the rise of containerized workloads in datacenters, Kubernetes has firmly
become the leading platform for container orchestration. It has experienced
widespread adoption across diverse industries and organizations of all
scales~\cite{kubernetes-adoption-statistics} and serves as the infrastructure
backbone for cloud computing services like Google Cloud~\cite{google-gke} and
Azure~\cite{azure-aks}. Its versatility, from supporting machine learning (ML)
deployments~\cite{kubernetes-ai}, edge computing~\cite{cloudraft} end serverless
functions~\cite{knative, openwhisk}, solidifies Kubernetes' position as a
cornerstone of the cloud-native industry. For these large-scale and dynamic
environments, efficient scheduling is paramount, as wasted resources can
directly impact an organization's bottom line Misconfiguration in resource
requests and limits is a primary cause of under-utilised resources or idle
nodes, leading to higher billing \cite{cost-strategies,
bin-packing-and-cost-savings-in-kubernetes-clusters-on-aws}.

Kubernetes' default scheduler~\cite{kube-scheduler} (\texttt{kube-scheduler})
performs bin-packing based on the resource requests included in Pod descriptions
and heuristic-based capacities calculated from the Nodes' advertised hardware.
Therefore, \texttt{kube-scheduler}'s performance relies on accurate estimates
of Pod resource usage. While telemetry-based systems have been proposed to
improve scheduling decisions, they often use expensive deep-ML techniques
\cite{bao2019deep, peng2021dl2} or focuses on a per-Node
level~\cite{yang2019design}. These telemetry-based systems typically augment
existing resource requests or re-schedule already running workloads to less
congested Nodes.

\textsc{Pronto}~\cite{grammenos_pronto_2021} leveraging of telemetry across a
all nodes offers a promising approach for a purely telemetry-driven Kubernetes
scheduler. However, its direct application within a Kubernetes scheduling
environment faces significant challenges. \textsc{Pronto}'s original design
assumes zero communication latency and yields only a binary "Reject-Job" signal,
which lacks the granularity for Node scoring and fails to account
for significant Pod startup latencies observed in real-world Kubernetes clusters
\cite{qadeer_scaling_2022}. Furthermore, empirical investigations into Linux
Pressure Stall Information (PSI) metrics, a potential replacement for the VMware
vSphere CPU-Ready metric, revealed frequent transient spikes attributed to the
container runtime. This would make it difficult to efficiently distinguish
genuine resource contention from noise when collecting a small set of metrics.
These limitations necessitate a more sophisticated signal that is both
\textbf{comparable} for effective Node ranking and \textbf{reservable} to track
the pending impact of in-flight Pods.

\section{Dissertation Aims and Contributions}

This dissertation aims to apply the theory behind \textsc{Pronto} to scheduling
in Kubernetes by proposing, implementing, and evaluating \textsc{CARICO} (Italian
for "load"), a novel, purely telemetry-driven scheduling algorithm that
maintains the federated, asynchronous, and memory-limited properties of
\textsc{Pronto} explicitly accounting for communication latency.

The key contributions of this project are:
\begin{itemize}
    \item \textbf{Feasibility Analysis of \textsc{Pronto} for Kubernetes:} An
        investigation into the applicability of \textsc{Pronto}'s principles to
        Kubernetes scheduling, highlighting its flawed communication assumptions and
        presenting empirical evidence of the challenges of using raw telemetry
        metrics.
    \item \textbf{Proposal of \textsc{CARICO}:} A novel federated, asynchronous,
        and memory-limited scoring algorithm that explicitly accounts for
        communication latency using a reservation mechanism.
    \begin{itemize}
        \item Novel application of Federated Singular Value Decomposition (FSVD)
            to build a local model of recent resource usage, providing new
            interpretations and a modified Subspace-Merge function.
        \item Presentation of a new \textbf{comparable} Capacity signal to score a Node's
            predicted ability to accept new workloads, considering recent
            resource utilisation across the cluster.
        \item Application of signal processing techniques to transform
            the generated capacity signal into a \textbf{reservable} metric,
            enabling schedulers to account for communication latencies.
    \end{itemize}
    \item \textbf{Prototype Implementation and Evaluation of \textsc{CARICO}:}
    \begin{itemize}
        \item Extensive investigation into different telemetry metrics and
            signal processing techniques within the setting of a Kubernetes
            cluster to generate an accurate and precise capacity signal.
        \item Analysis of the correctness of the generated signal and
            exploration of approaches for estimating reservation quantities.
        \item Comprehensive evaluation of \textsc{CARICO}'s overall performance
            under various workloads on a real-world Kubernetes cluster,
            comparing it with \texttt{kube-scheduler} across multiple scheduling
            objectives.
    \end{itemize}
\end{itemize}

\section{Dissertation Overview}
The remainder of this dissertation is structured as follows:
\begin{itemize}
    \item \textbf{Chapter 2 (Background):} Provides foundational
        knowledge, detailing Kubernetes architecture and its scheduling
        mechanism. It introduces \textsc{Pronto}, explaining its core
        mathematics and highlighting its accuracy in predicting performance
        degradation, while also critically examining its weaknesses that
        necessitate a more sophisticated signal. Finally, it reviews existing
        Kubernetes schedulers to contextualize this work within the current
        landscape.
    \item \textbf{Chapter 3 (Design):} Proposes the
        \textsc{CARICO} algorithm, providing detailed proofs and reasoning to
        explain the generation and properties of its capacity signal.
    \item \textbf{Chapter 4 (Implementation):} Outlines the
        structure of the \textsc{CARICO} system within Kubernetes, exploring the
        selection of numerous metrics and signal processing techniques employed
        to generate an accurate comparable and reservable signal.
    \item \textbf{Chapter 5 (Evaluation):} Presents a comprehensive
        evaluation of \textsc{CARICO}'s performance under different workloads,
        collecting numerous performance metrics and comparing it against the
        standard \texttt{kube-scheduler}.
    \item \textbf{Chapter 6 (Conclusion and Future Work):} Summarizes the key
        findings of the dissertation and proposes potential directions for
        future research.
\end{itemize}
