\chapter{Required Lemmas for Model Interpretation}

\section{Proof: The First Left Singular Vector of Non-Negative Matrices as a
Pseudo Weighted Average}
\label{app:vector-to-avg}
Let $\mathbf{A}$ be an $m \times n$ matrix with non-negative elements $0 \leq
a_{ij} \leq 1$. Let $u_1$ be the first left singular vector and $\sigma_1$ the
corresponding singular value of $\mathbf{A}$.

\subsection{Fundamental Representation of $u_1$}
The vector $u_1$ is an eigenvector of $\mathbf{AA}^T$ associated with the
largest eigenvalue $\sigma_1^2$: $\mathbf{AA}^T = \sigma_1^2 u_1$. Let $a_j$
denote the $j$-th column of $\mathbf{A}$. Then $\mathbf{AA}^T = \sum_{j=1}^n a_j
a_j^T$. Substituting this into the eigenvalue equation and assuming $\sigma_1
\neq 0$: $\big( \sum_{j = 1}^n a_j a_j^T\big) u_1 = \sigma_1^2 u_1 \implies
\sum_{j=1}^n a_j (a_j^T u_1) = \sigma_1^2 u_1$. Thus $u_1$ can be expressed as a
linear combination of the columns of $\mathbf{A}$:
\[ u_1 = \sum_{j=1}^n \bigg(\frac{a_j^T u_1}{\sigma_1^2}\bigg) a_j =
\sum_{j=1}^n w_j a_j, \text{  where } w_j = \frac{a_j^T u_1}{\sigma_1^2} \]

\subsection{Contribution of Columns to $u_1$}
The magnitude of each weight $w_j$ is given by:
\[ |w_j| = \frac{|a_j^T u_1|}{\sigma_1^2} = \frac{\parallel a_j\parallel
|\cos(\theta_j)|}{\sigma_1^2}\]
where $\theta_j$ is the angle between column $a_j$ and $u_1$. Columns $a_j$ with
larger norms $\parallel a_j \parallel$ or those more parallel to $u_1$ (i.e.
$|\cos(\theta_j)| \approx 1$) will have weights $w_j$ of greater magnitude.
Consequently, these "larger" or more aligned columns contribute more
significantly to the sum defining $u_1$ and thus to its direction

\subsection{Non-Uniqueness of $u_1$ and Relation to Perron-Frobenius Theorem}
The singular vectors in SVD are not unique: if $\mathbf{A} =
\mathbf{U}_1\Sigma\mathbf{V}_1^T = \mathbf{U}_2\Sigma\mathbf{V}_2^T$ then
$\Sigma_1 = \Sigma_2$ but $\mathbf{U}_1 = \mathbf{U}_2\mathbf{B}_a$ and
$\mathbf{V}_1 = \mathbf{V}_2\mathbf{B}_b$ for some block diagonal unitary
matrices $\mathbf{B}_a, \mathbf{B}_b$~\cite{eftekhari2019moses}.

Since $\mathbf{A}$ has non-negative entries ($a_{ij} \geq 0$), $\mathbf{AA}^T$
is a non-negative matrix. By the Perron-Frobenius theorem, there exists an
eigenvector $u_1^+$ corresponding to the eigenvalue $\sigma_1^2$ whose
components $u_{1i}^+$ are all non-negative ($u_{1i}^+ \geq 0$). The first left
singular vector $u_1$ obtained from SVD must then be $u_1 = bu_1^+$ where $b =
\pm 1$

\subsection{Interpretation as a Pseudo Weighted Average}
\begin{itemize}
    \item \textbf{Case 1:} $b = 1 \implies u_1 = u_1^+$.\\
    In this case, $a_j^Tu_1 = a_j^Tu_1^+ = \sum_i a_{ij}u_{1i}^+ \geq 0$ (as
    $a_{ij} \geq 0, u_{1i}^+ \geq 0$).\\
    The weights $w_j = (a_j^Tu_1)/\sigma_1^2$ are therefore non-negative ($w_j
    \geq 0$).\\
    $u_1 = \sum w_ja_j$ becomes a conic combination of the non-negative column
    vectors $a_j$. This $u_1$ (itself non-negative) acts as a pseudo weighted
    average, representing a principal direction within the cone spanned by the
    $a_j$. Columns contributing larger non-negative $w_j$ pull $u_1$ more
    strongly in their direction.
    \item \textbf{Case 2:} $b = -1 \implies u_1 = -u_1^+$.\\
    Here, $a_j^Tu_1 = a_j^T(-u_1^+) =  - a_j^T(u_1^+) \leq 0$.\\
    The weights $w_j = (a_j^Tu_1)/\sigma_1^2$ are non-positive ($w_j \leq 0$).\\
    While $u_1 = \sum w_ja_j$ now involves non-positive weights for non-negative
    vectors $a_j$, the magnitudes $|w_j| = (a_j^Tu_1)/\sigma_1^2$ remain the
    same as in Case 1. Thus, columns $a_j$ that are ``larger" or more aligned
    with $u_1^+$ still contribute with greater magnitude to the sum, determining
    the orientation of the line spanned by u.
\end{itemize}

\subsection{Conclusion}
The vector $u_1$ is a linear combination of the columns of $\mathbf{A}$, $u_1 =
\sum w_j a_j$. The magnitude of the coefficient $w_j$ for each column $a_j$ is
proportional to the projection of $a_j$ onto $u_1$ (scaled by $1/\sigma_1^2$).
This means the columns with larger norms or those more aligned with the
principal direction (the line spanned by $u_1$) contribute more significantly to
defining this direction.

Irrespective of the sign $b$ (determined by the SVD algorithm), the line along
which $u_1$ lies is shaped by this weighted aggregation.

\section{Proof: Monotonicity of the First Singular Value for Non-Negative Matrices}
\label{sec:app-monotonicity}
Let $\mathbf{A}$ and $\mathbf{B}$ be $m \times n$ matrices with real entries
such that $0 \leq a_{ij} \leq b_{ij} \leq 1$ for all $i,j$. We want to show that
$\sigma_1(\mathbf{A}) \leq \sigma_1(\mathbf{B})$, where $\sigma_1(\mathbf{M})$
denotes the first (largest) singular value of a matrix $\mathbf{M}$.

\subsection{Relating Singular Values to $\mathbf{M}^T\mathbf{M}$}
\label{sec:mono-one}
The square of the first singular value, $\sigma_1(\mathbf{M})^2$, is the largest
eigenvalue of the matrix $\mathbf{M}^T\mathbf{M}$. Since
$\mathbf{M}^T\mathbf{M}$ is a symmetric positive semi-definite matrix, its
largest eigenvalue is also its spectral radius, $\rho(\mathbf{M}^T\mathbf{M})$.
Thus, $\rho(\mathbf{A})^2 = \rho(\mathbf{A}^T\mathbf{A})$ and
$\rho(\mathbf{B})^2 = \rho(\mathbf{B}^T\mathbf{B})$.

\subsection{Comparing $\mathbf{A}^T\mathbf{A}$ and $\mathbf{B}^T\mathbf{B}$}
Let $\mathbf{M}_{\mathbf{A}} = \mathbf{A}^T\mathbf{A}$ and
$\mathbf{M}_{\mathbf{B}} = \mathbf{B}^T\mathbf{B}$. The $(j,k)$-th element of
thess matrices are:
\begin{align}
    (\mathbf{M}_{\mathbf{A}})_{jk} &= \sum_{i=1}^m a_{ij}a_{ik} \\
    (\mathbf{M}_{\mathbf{B}})_{jk} &= \sum_{i=1}^m b_{ij}b_{ik}
\end{align}

Since $0 \leq a_{ij} \leq b_{ij}$ for all $i,j$:
\begin{itemize}
    \item All $a_{ij}$ and $b_{ij}$ are non-negative
    \item Therefore, $a_{ij}a_{ik} \leq b_{ij}b_{ik}$ for all $i,j,k$. Summing
        over $i$:
        \[ \sum_{i=1}^m a_{ij}a_{ik} \leq \sum_{i=1}^m b_{ij}b_{ik} \]
        This implies $(\mathbf{M}_{\mathbf{A}})_{jk} \leq
        (\mathbf{M}_{\mathbf{B}})_{jk}$ for all $j,k$. Thus, $0 \leq
        \mathbf{M}_{\mathbf{A}} \leq \mathbf{M}_{\mathbf{B}}$ element-wise. Both
        $\mathbf{M}_{\mathbf{A}}$ and $\mathbf{M}_{\mathbf{B}}$ are matrices
        with non-negative entries.
\end{itemize}

\subsection{Monotonicity of Spectral Radius}
\label{sec:mono-three}
A standard result from Perron-Frobenius theory states that if $\mathbf{X}$ and
$\mathbf{Y}$ are non-negative matrices such that $\mathbf{X} \leq \mathbf{Y}$
element-wise ($\forall i,j. x_{ij} \leq y_{ij}$), then their spectral radii satisfy $\rho(\mathbf{X}) \leq
\rho(\mathbf{Y})$. Applying this result to $\mathbf{M}_{\mathbf{A}} =
\mathbf{A}^T\mathbf{A}$ and $\mathbf{M}_{\mathbf{B}} =
\mathbf{B}^T\mathbf{B}$:
\[ \rho(\mathbf{A}^T\mathbf{A}) \leq \rho(\mathbf{B}^T\mathbf{B}) \]

\subsection{Conclusion}
From Section \ref{sec:mono-one} and \ref{sec:mono-three}:
\[ \sigma_1(\mathbf{A})^2 \leq \sigma_1(\mathbf{B})^2 \]
Since the singular values are by definition non-negative ($\sigma_1(\mathbf{M})
\geq 0$):
\[ \sigma_1(\mathbf{A}) \leq \sigma_1(\mathbf{B}) \]
This shows that if the values of a non-negative matrix $\mathbf{A}$ are
increased (while remaining non-negative, e.g., elements between $0$ and $1$),
the first singular value of the resulting matrix $\mathbf{B}$ wil greater than
or equal to that of $\mathbf{A}$.

\section{Proof: First Singular Value Behaviour in Non-Negative Concatenated
Matrices}
\label{sec:app-concatenate}
This section proves the following statement:\\
Given two non-negative matrices $\mathbf{A} \in \mathbb{R}^{m \times n_1}$ and
$\mathbf{B} \in \mathbb{R}^{m \times n_2}$, the first singular value of the
concatenated matrix $\mathbf{C} = [\mathbf{A}, \mathbf{B}]$ is greater than or
equal to the maximum of the first singular values of $\mathbf{A}$ and
$\mathbf{B}$. That is,
\[ \sigma_1([\mathbf{A}, \mathbf{B}]) \geq \max(\sigma_1(\mathbf{A}),
\sigma_1(\mathbf{B})) \]

\subsection{Setup}
Let $\mathbf{A}$ be an $m \times n_1$ matrix and $\mathbf{B}$ be an $m \times
n_2$ matrix. The concatenated matrix $\mathbf{C} = [\mathbf{A}, \mathbf{B}]$ is
an $m \times (n_1 + n_2)$ matrix. The first singular value of any matrix
$\mathbf{M}$, denoted $\sigma_1(\mathbf{M})$, is defined as its spectral norm:
\[ \sigma_1(\mathbf{M}) = \parallel \mathbf{M} \parallel_2 = \max_{\parallel x
\parallel_2 = 1} \parallel \mathbf{M}x \parallel_2 \]
where $x$ is a unit vector.

\subsection{Relating $\sigma_1(\mathbf{C})$ to $\sigma_1(\mathbf{A})$}
\label{sec:bigger-than-A}
Let $v_{\mathbf{A}}^*$ be a unit vector in $\mathbb{R}^{n_1}$ such that
$\sigma_1(\mathbf{A}) = \parallel \mathbf{A}v_{\mathbf{A}}^* \parallel_2$. Such
a vector $v_{\mathbf{A}}^*$ is the right singular vector corresponding to
$\sigma_1(\mathbf{A})$.

Consider a vector $x_0 \in \mathbb{R}^{n_1 + n_2}$ constructs as $x_0 =
\begin{bmatrix} v_{\mathbf{A}}^* \\ 0_{n_2 \times 1} \end{bmatrix}$, where
$0_{n_2 \times 1}$ is the zero vector of dimensions $n_2$.\\
The squared norm of $x_0$ is $\parallel x_0 \parallel_2^2 = \parallel
v_{\mathbf{A}}^* \parallel_2^2 + \parallel 0_{n_2 \times 1} \parallel_2^2 = 1^2
+ 0  = 1$. Thus, $x_0$ is a unit vector.

By definitions of $\sigma_1(\mathbf{C})$:
\[ \sigma_1(\mathbf{C}) = \max_{\parallel x \parallel_2 = 1} \parallel
\mathbf{C}x \parallel_2 \]
Since $x_0$ is a specific unit vector, we have:
\[ \sigma_1(\mathbf{C}) \geq \parallel \mathbf{C}x_0 \parallel_2 \]
Computing $\mathbf{C}x_0$ gives:
\[ \mathbf{C}x_0 = [\mathbf{A}, \mathbf{B}] \begin{bmatrix} v_{\mathbf{A}}^* \\
0_{n_2 \times 1} \end{bmatrix} = \mathbf{A}v_{\mathbf{A}}^* + \mathbf{B} \dot
0_{n_2 \times 1} = \mathbf{A}v_{\mathbf{A}}^* \]
Therefore,
\[ \sigma_1(\mathbf{C}) \geq \parallel \mathbf{A}v_{\mathbf{A}}^* \parallel_2 =
\sigma_1(\mathbf{A})\]

\subsection{Relating $\sigma_1(\mathbf{C})$ to $\sigma_1(\mathbf{B})$}
\label{sec:bigger-than-B}
Similarly, let $v_{\mathbf{B}}^*$ be a unit vector in $\mathbb{R}^{n_2}$ such that
$\sigma_1(\mathbf{B}) = \parallel \mathbf{B}v_{\mathbf{B}}^* \parallel_2$. Such

Consider a vector $x_1 \in \mathbb{R}^{n_1 + n_2}$ constructs as $x_1 =
\begin{bmatrix} 0_{n_1 \times 1} \\ v_{\mathbf{B}}^* \end{bmatrix}$.\\
The squared norm of $x_1$ is $\parallel x_1 \parallel_2^2 = \parallel 0_{n_1
\times 1} \parallel_2^2 + \parallel v_{\mathbf{B}}^* \parallel_2^2 = 0 + 1^2 =
1$. Thus, $x_1$ is a unit vector.

Again, by definitions of $\sigma_1(\mathbf{C})$:
\[ \sigma_1(\mathbf{C}) \geq \parallel \mathbf{C}x_1 \parallel_2 \]
Computing $\mathbf{C}x_1$ gives:
\[ \mathbf{C}x_1 = [\mathbf{A}, \mathbf{B}] \begin{bmatrix} 0_{n_1 \times 1} \\
v_{\mathbf{B}}^* \end{bmatrix} = \mathbf{A} \dot 0_{n_1 \times 1} + \mathbf{B}
v_{\mathbf{B}}^* = \mathbf{B}v_{\mathbf{B}}^* \]
Therefore,
\[ \sigma_1(\mathbf{C}) \geq \parallel \mathbf{B}v_{\mathbf{B}}^* \parallel_2 =
\sigma_1(\mathbf{B})\]

\subsection{Conclusion}
From Sections \ref{sec:bigger-than-A} and \ref{sec:bigger-than-B}, we have shown
that $\sigma_1(\mathbf{C}) \geq \sigma_1(\mathbf{A})$ and $\sigma_1(\mathbf{C})
\geq \sigma_1(\mathbf{B})$. This implies that $\sigma_1(\mathbf{C})$ must be
greater than or equal to the maximum of these two values:
\[ \sigma_1([\mathbf{A}, \mathbf{B}]) \geq \max(\sigma_1(\mathbf{A}),
\sigma_1(\mathbf{B})) \]

\section{Damped Merging: First Singular Value Behaviour with Non-Negative
Weighted Concatenated Matrices}
\label{sec:app-scale}
This section proves the following statement:
Given two non-negative matrices $\mathbf{A} \in \mathbb{R}^{m \times n_1}$ and
$\mathbf{\mathbf{B}} \in \mathbb{R}^{m \times n_2}$ and non-negative scalar weights
$\gamma_{\mathbf{A}} = \sqrt{w_{\mathbf{A}}}$ and $\gamma_{\mathbf{\mathbf{B}}} =
\sqrt{w_{\mathbf{\mathbf{B}}}}$ such that $w_{\mathbf{A}} + w_{\mathbf{\mathbf{B}}} = 1$, the
resulting first singular value and the first left
singular vector of the concatenated matrix $\mathbf{C} = [\gamma_{\mathbf{A}}\mathbf{A},
\gamma_{\mathbf{\mathbf{B}}}\mathbf{\mathbf{B}}]$ has the following property:
\[ \min(P_{\mathbf{A}}(u_{\mathbf{C}}),P_{\mathbf{\mathbf{B}}}(u_{\mathbf{C}})) \leq \sigma_{\mathbf{C}}^2 \leq
\max(P_{\mathbf{A}}(u_{\mathbf{C}}),P_{\mathbf{\mathbf{B}}}(u_{\mathbf{C}})) \]
where $\sigma_{\mathbf{C}}$ and $u_{\mathbf{C}}$ correspond to the first singular value and first
left singular vector of $\mathbf{C}$, $P_{\mathbf{M}}(u)$ is the sum of squared scalar projections
of the columns in $\mathbf{M}$ onto $u$ (let $m_j$ be the $j$-th column of $\mathbf{M}$ $P_{\mathbf{M}}(u)
= \sum_{j=1}^n (u^T m_j)^2$).

\subsection{Expanding $\sigma_1(\mathbf{C})$}
By definition, $S_{\mathbf{C}} = \sigma_1(\mathbf{C})$ and $u_{\mathbf{C}}$ is the corresponding first
singular left vector. Thus, they satisfy the eigenvalue equation for $\mathbf{C}\mathbf{C}^T$:
\[ \mathbf{C}\mathbf{C}^Tu_{\mathbf{C}} = S_{\mathbf{C}}^2 u_{\mathbf{C}} \]
Pre-multiplying by $u_{\mathbf{C}}^T$:
\begin{align}
    u_{\mathbf{C}}^T(\mathbf{C}\mathbf{C}^T u_{\mathbf{C}}) &= u_{\mathbf{C}}^T(S_{\mathbf{C}}^2 u_{\mathbf{C}}) \\
    u_{\mathbf{C}}^T \mathbf{C}\mathbf{C}^T u_{\mathbf{C}} &= S_{\mathbf{C}}^2 (u_{\mathbf{C}}^T u_{\mathbf{C}})
\end{align}
Since $u_{\mathbf{C}}$ is a unit vector, $u_{\mathbf{C}}^T u_{\mathbf{C}} = \parallel u_{\mathbf{C}} \parallel_2^2 =
1$. So,
\[ S_{\mathbf{C}}^2 = u_{\mathbf{C}}^T \mathbf{C}\mathbf{C}^T u_{\mathbf{C}} \]
Now, lets express $\mathbf{C}\mathbf{C}^T$ in terms of $\mathbf{A}$ and $\mathbf{B}$:\\
The matrix $\mathbf{C} = [\gamma_{\mathbf{A}}\mathbf{A},\gamma_{\mathbf{B}}\mathbf{B}]$.\\
Then $\mathbf{C}^T = \begin{bmatrix} \gamma_{\mathbf{A}}\mathbf{A}^T \\ \gamma_{\mathbf{B}}\mathbf{B}^T \end{bmatrix}$.\\
So, $\mathbf{C}\mathbf{C}^T = [\gamma_{\mathbf{A}}\mathbf{A},\gamma_{\mathbf{B}}\mathbf{B}]\begin{bmatrix} \gamma_{\mathbf{A}}\mathbf{A}^T \\
\gamma_{\mathbf{B}}\mathbf{B}^T \end{bmatrix} = (\gamma_{\mathbf{A}}\mathbf{A})(\gamma_{\mathbf{A}}\mathbf{A}^T) +
(\gamma_{\mathbf{B}}\mathbf{B})(\gamma_{\mathbf{B}}\mathbf{B}^T)$
\[ \mathbf{C}\mathbf{C}^T = \gamma_{\mathbf{A}}^2\mathbf{A}\mathbf{A}^T + \gamma_{\mathbf{B}}^2\mathbf{B}\mathbf{B}^T \]
Substitute this expression for $\mathbf{C}\mathbf{C}^T$ into the equation for $S_{\mathbf{C}}^2$:
\begin{align}
    S_{\mathbf{C}}^2 &= u_{\mathbf{C}}^T (\gamma_{\mathbf{A}}^2\mathbf{A}\mathbf{A}^T + \gamma_{\mathbf{B}}^2\mathbf{B}\mathbf{B}^T)u_{\mathbf{C}} \\
    S_{\mathbf{C}}^2 &= \gamma_{\mathbf{A}}^2(u_{\mathbf{C}}^T\mathbf{A}\mathbf{A}^Tu_{\mathbf{C}}) + \gamma_{\mathbf{B}}^2(u_{\mathbf{C}}^T\mathbf{A}\mathbf{A}^Tu_{\mathbf{C}})
\end{align}
Using the definitions for $P_{\mathbf{A}}(u_{\mathbf{C}})$ and $P_{\mathbf{B}}(u_{\mathbf{C}})$:
\begin{itemize}
    \item $P_{\mathbf{A}}(u_{\mathbf{C}}) = u_{\mathbf{C}}^T\mathbf{A}\mathbf{A}^Tu_{\mathbf{C}}$ (sum of squared projections of columns of $\mathbf{A}$ onto $u_{\mathbf{C}}$
    \item $P_{\mathbf{B}}(u_{\mathbf{C}}) = u_{\mathbf{C}}^T\mathbf{B}\mathbf{B}^Tu_{\mathbf{C}}$ (sum of squared projections of columns of $\mathbf{B}$ onto $u_{\mathbf{C}}$
\end{itemize}
This means the equation becomes:
\[ S_{\mathbf{C}}^2 = \gamma_{\mathbf{A}}^2P_{\mathbf{A}}(u_{\mathbf{C}}) + \gamma_{\mathbf{B}}^2P_{\mathbf{B}}(u_{\mathbf{C}}) \]
We are given weights $\gamma_{\mathbf{A}}, \gamma_{\mathbf{B}}$ such that $\gamma_{\mathbf{A}}^2 \geq 0,
\gamma_{\mathbf{B}}^2 \geq 0$, and $\gamma_{\mathbf{A}}^2 + \gamma_{\mathbf{B}}^2 = 1$. This means that
$S_{\mathbf{C}}^2$ is a convex combination of the two values $P_{\mathbf{A}}(u_{\mathbf{C}})$ and
$P_{\mathbf{B}}(u_{\mathbf{C}})$.
A fundamental property of convex combinations is that they always lie between
the minimum and maximum of the values being combined.\\
Let $x = P_{\mathbf{A}}(u_{\mathbf{C}})$ and $y = P_{\mathbf{B}}(u_{\mathbf{C}})$. Then $S_{\mathbf{C}}^2 = \gamma_{\mathbf{A}}^2 x +
\gamma_{\mathbf{B}}^2 y$.\\
If $x \leq y$:
\begin{itemize}
    \item $\min(x,y) = x = (\gamma_{\mathbf{A}}^2 + \gamma_{\mathbf{B}}^2)x = \gamma_{\mathbf{A}}^2 x +
        \gamma_{\mathbf{B}}^2 x \leq \gamma_{\mathbf{A}}^2 x + \gamma_{\mathbf{B}}^2 y = S_{\mathbf{C}}^2$ (since
        $\gamma_{\mathbf{B}}^2 \geq 0$ and $x \leq y$).
    \item $\max(x,y) = y = (\gamma_{\mathbf{A}}^2 + \gamma_{\mathbf{B}}^2)y = \gamma_{\mathbf{A}}^2 y +
        \gamma_{\mathbf{B}}^2 y \geq \gamma_{\mathbf{A}}^2 x + \gamma_{\mathbf{B}}^2 y = S_{\mathbf{C}}^2$ (since
        $\gamma_{\mathbf{A}}^2 \geq 0$ and $x \leq y$).
\end{itemize}
A similar argument holds if $y \leq x$. Therefore:
\[ \min(P_{\mathbf{A}}(u_{\mathbf{\mathbf{C}}}),P_{\mathbf{\mathbf{B}}}(u_{\mathbf{\mathbf{C}}})) \leq \sigma_{\mathbf{\mathbf{C}}}^2 \leq
\max(P_{\mathbf{A}}(u_{\mathbf{\mathbf{C}}}),P_{\mathbf{\mathbf{B}}}(u_{\mathbf{\mathbf{C}}})) \]

\chapter{Evaluation Setup}

\section{Scheduling Objectives}
\label{app:setup-objectives}
\textbf{Job Completion - Section \ref{sec:eval-job-completion}:}\\
Schedulers often aim to reduce the time it takes for a job or a set of jobs to
complete. In Kubernetes, Jobs are specified by a template of the Pods to be
executed, the total number of Pods to complete (\texttt{completions}) and the
maximum number of Pods running at once (\texttt{parallelism}). Job completion
time (JCT) is defined as the time between a Job object being published to the
Kubernetes API and the time when the last Pod of the Job completes. During the
experiments, I set \texttt{parallelism} to \texttt{completions} so that only
\texttt{kube-scheduler}'s decisions impact Job completion.

\textbf{Pod Completion - Section~\ref{sec:eval-pod-completion}:}\\
Another common goal is to reduce the time it takes for idividual tasks to complete.
This section investigates the distribution of Pod Completion time (PCT): the
time it takes from when a Pod starts running to when it completes. In addition,
traces of the number of concurrently running Pods on each Node during Job
execution are used to explain the PCTs.

\textbf{Resource Utilisation - Section~\ref{sec:eval-util}:}\\
Since resources are expensive, data canters aim to ensure that resources are
well utilised with efficient placements. Over-utilisation can result in large
amounts of resource thrashing, which wastes resources and hurts a data canters
profitability. As \textsc{Carico} currently only uses CPU and memory metrics
for scheduling, only the utilisation of those resources are measured during the
execution of Jobs.

\textbf{Workload Heterogeneity - Section~\ref{sec:eval-hetero}:}\\
As outlined in \ref{sec:background-datacenters}, data canters must handle
workloads with different characteristics, such as, resource usage and running
times. This section evaluates \textsc{Carico} performance with multiple Jobs
with different characteristics.

\textbf{Workload Isolation - Section~\ref{sec:eval-isolation}:}\\
Data canters have to deal with jobs with different QoS requirements. One of
these requirements is workload isolation: whether the execution of one Job will
interfere with the execution of another, impacting a user's experience. This
experiment measures the impact on a scheduling decisions on the response latency
of a server running in the cluster.

\textbf{Overhead - Section~\ref{sec:eval-overhead}:}\\
Due to the limited number of resources that need to be shared between users,
schedulers must mitigate their overhead. A lower overhead frees up more
resources for users and providers can achieve higher profits margins. This
experiment measures the overhead incurred from running the DaemonSet of
\textsc{Carico} Pods.


\section{Cluster Setup}
\label{app:setup-cluster}

These experiments ran on a Kubernetes cluster containing 20 virtual machines
(VMs) running on the Xen hypervisor. One of the machines is used as the master
Node, and the rest are worker Nodes. The master Node contains all the Pods in
the control plane, and during the evaluation of \textsc{Carico}, it contains the \textsc{Carico}
Scheduler and Aggregation Server. Each VM advertises four Intel Xeon Gold 6142
CPUs \@ 2.60GHz with 8 GB of RAM running Ubuntu 24.04.2 LTS. Each CPU has a
single core with hyper-threading disabled. When running \texttt{kubectl describe
nodes}, each Node advertises $4000$ milliCPU seconds and 8GB of RAM.

During the evaluation, I use a Prometheus deployment \cite{turnbull2018monitoring} to collect various
statistics, such as running Pod count, resource utilisation and Kubernetes
object completions.

%
% For any practical projects, you should almost certainly have some kind
% of evaluation, and it's often useful to separate this out into its own
% chapter.

\section{Experimental Workloads}
\label{app:setup-workloads}
During the experiments I used very short-lived workloads; workloads that take
less than a minute to complete. While data canters can expect to receive
longer-running workloads, using them in the evaluation was not feasible due to
the need to run experiments multiple times and limited time. Short-lived tasks
can still provide valuable insights into the performance of a scheduler as their
short completion time makes the impact of poor scheduling decisions more
significant and easier to observe.

During this evaluation, I used two types of workloads:
\begin{enumerate}
    \item \texttt{pi-2000}: A short-lived CPU-focused workload where a Pod
        computes the value of $\pi$ to 2000 decimal points.
    \item \texttt{sklearn}: A longer-running workload with a larger memory
        footprint. This Pod executes a script which uses
        \texttt{sklearn} which trains a small neural network classifier ($512$
        features, $16$ classes, $8$ hidden layers each containing $1024$
        neurons) on 5000 randomly generated samples before running inference on
        another set of 5000 randomly generated samples.
\end{enumerate}
Investigating metrics when running \texttt{pi-2000} and \texttt{sklearn} Jobs
separately demonstrates how \textsc{Carico} handles opposing resource usage
characteristics (CPU-focused vs memory-focused). Furthermore, when executing
both these Jobs concurrently, the difference in runtime helps investigate how
\textsc{Carico} also handles workloads with different running times.

% In this experiment, I use a Job that specifies Pods that performed a small ML
% workload. This workload uses a significant amount of memory, which unlike CPU,
% must be carefully handled. If we increase the number of processes on a
% fully-utilised CPU, it only results in each process having a smaller portion of
% CPU time and degrading its performance. On the other hand, memory is less
% forgiving as once memory runs out, the kernel begins OOM killing processes. This
% be detrimental to Job Completion, as terminated Pods results in wasted
% computations.
%


