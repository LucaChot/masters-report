\chapter{Background}
% A more extensive coverage of what's required to understand your work.

% In general you should assume the reader has a good undergraduate
% degree in computer science, but is not necessarily an expert in the
% particular area you've been working on. Hence this chapter may need to
% summarize some ``text book'' material.
%
% This is not something you'd normally require in an academic paper, and
% it may not be appropriate for your particular circumstances. Indeed,
% in some cases it's possible to cover all of the ``background''
% material either in the introduction or at appropriate places in the
% rest of the dissertation.
%

\section{Kubernetes Overview}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/components-of-kubernetes.pdf}
    \caption{The components of a Kubernetes cluster. \textbf{Source:}
    \cite{kubernetes-components}}
    \label{kube-components}
\end{figure}

A Kubernetes cluster consists of a control plane and one or more worker nodes.
Components in the control plane manage the overall state of the cluster. The
\verb|kube-apiserver| exposes the Kubernetes HTTP API, which is used to publish
objects such as Deployments, DaemonSets and Jobs. The \verb|kube-server| looks
for pods not yet bound to a pod, and assigns each Pod to a suitable node. Each
node in the cluster contains a set of components which maintain the running pods
and provide the Kubernetes runtime environment. \verb|kubelet| manages Pods and
ensures they and their containers are running. The \verb|kubelet| uses a
container runtime: software responsible for running containers.

Kubernetes objects are persistent entities in the Kubernetes system. They act as
"records of intent" and describe the cluster's desired state: once created, the
Kubernetes system will constantly work to ensure that the objects exists. The
Kubernetes API is used to create, modify or delete Kubernetes objects. Almost
every Kubernetes object includes two fields: \verb|spec| and \verb|status|.
\verb|spec| is used on creation as a description of the characteristics you want
the resource to have, while \verb|status| describes the current state of the
object, supplied and updated by the Kubernetes system. These fields are crucial
for scheduling pods as they behave like resource constraints, with which the
Kubernetes schedulers can use to determine optimal pod placement.

\section{Scheduling in Kubernetes}
In Kubernetes, Pods are the smallest deployable units of computing that you can
create and manage in Kubernetes. It represents a single instance of a running
process in your cluster and typically contains one or more containers that are
tightly coupled and share resources. Pods can be individually created with their
own Yaml files. However, the Kubernetes API also provides workload objects to manage
multiple pods: these objects repressent a higher abstraction level than a Pod,
and the Kubernetes control plane uses the workload's specification to manage Pod
objects on your behalf. The built-in APIs for managing workloads include
Deployment, StatefulSet, DaemonSet and Job.

When a Pod is created, it initially exists in a "Pending" state - it has been
declared by hasn't been allocated to a Node. Kubernetes schedulers watch for
newly created Pods that have yet been assigned to a Node, and based on a set of
rules or algorithms, select the most suitable Node for that Pod. Once a Node is
chosen, the scheduler "binds" the Pod to the Node, updating the Pod's definition
in the Kubernetes API server by setting its \verb|spec.nodeName| field to the
name of the Node. Once this occures, the Pod transitions from "Pending" to
"Running".

\section{Analysis of Scheduling Approaches and QoS-Aware Trends}
Different Kubernetes schedulers take varied approaches to Pod placement, from
simple heuristics to global optimisations and machine learning. This diversity
reflects the wide range of workload requirements and cluster scales seen in
practice. In the following subsections, we compare how different schedulers
leverage different data inputs for decision-making, which fundamentally
influences their scheduling strategy and resulting quality of service (QoS) for
workloads.

\subsection{Pod Spec-Only Scheduling vs Telemetry-Aware Scheduling}

\textbf{Pod Description-Based schedulers} rely solely on the information
declared in the pod specification and cluster configuration (static data). The
default Kubernetes scheduler \cite{kube-scheduler} and similar schedulers like
Yunikorn \cite{cloudera_yunikorn_2019} or Volcano \cite{volcano_introduction}
fall in this category. They consider factors such as requested CPU/memory,
labels and selectors, affinity/anti-affinity rules, and priority. This approach
is more reliable for ensuring that the declared needs of a pod are met. For
example, a pod will only be placed on a node that has enough free capacity
according to a resource requests, and it will honour policies like zone
spreading or node affinity.

Benefits of this method are predictability and simplicity: by using static
reservations, the scheduler guarantees that if every pod accurately requests
its needs, no node will be over-committed at scheduling time. This can protect
critical workloads by giving them guaranteed resources (as in Kubernetes
Guaranteed QoS class). It also enables fairness policies at high level (e.g.
Yunikorn's queues and priorities ensure no team monopolises the cluster).

However, the downside is a lack of visibility into actual runtime conditions -
the scheduler is blind to fluctuations in load or hardware state. As a result,
static-only scheduling may lead to less optimal placements: for instance, a new
pod might be placed on a node that technically has room (per requests) but is
currently experience high CPU utilisation from other processes, leading to
contention. Meanwhile, an underutilised node could be ignored because its
capacity appears full due to over-requested resources or sticky allocations. In
summary, purely \verb|spec|-based scheduling treats the cluster as a static
partitioning of resources; it ensures each workload's declared needs are met, but
it can't respond to real-time performance variations.

\textbf{Telemetry-Based schedulers}, incorporate live metrics and feedback from
the cluster's current state to make more informed decisions. These schedulers
actively monitor CPU usage, memory pressure, I/O rates, cache misses, network
latency, etc., either via the Kubernetes Metrics APU, Prometheus, or custom
telemetry pipelines. Examples include Intel's Telemetry Aware Scheduling \cite{intel-tas}
(which uses fine-grained platform metrics like LLC cache hit rates and CPU
temperature) and the Trimaran plugin suite for Kubernetes \cite{trimaran} that uses
real-time utilisation stats.

By using this data, telemetry-driven scheduling can avoid placing pods on nodes
that are struggling or overloaded in real-time. For instance, a telemetry-aware
scheduler might detect that Node A's CPU is 90\% busy and Node B's is 20\% and
therefore prefer Node B for a new pod - even if, by static allocation, both
nodes had equal allocable capacity remaining. This dynamic adaptability
directly improves QoS: workloads get more stable performance because they are
less likely to be scheduled to an over-taxed machine. Moreover, telemetry-based
rules can target specific QoS concerns: Intel's TAS can steer latency-sensitive
network functions away from nodes with high cache thrashing, preventing
unpredictable latency spikes \cite{intel-tas}. The benefits of telemetry awareness is
higher efficiency and resiliency - clusters can run closer to their true capacity
(Trimaran's TargetLoadPacking can pack workloads until it observes
near-saturation) but back off when needed to maintain performance. It also helps
in maintaining SLAs by reacting to early warning signals before they become
failures.

The challenge, however, is complexity: collecting and reacting to metrics adds
overhead and potential instability (the scheduler decisions are now as dynamic
as the metrics themselves). Careful design is required to smooth out decisions.
telemetry-based scheduling brings an automated, feedback-driven approach that
can greatly boost performance and utilisation, but it must be tuned to avoid
thrashing and to scale metric collection overhead.

\textbf{Hybrid Approaches} combine both static pod descriptions and dynamic
telemetry to get the best of both worlds: guarantee the fundamental resource
requests/constraints from pod specs while using telemetry to fine-tune the
choice among feasible nodes. For example, Firmament (via Poseidon)
\cite{gog_firmament_2016, poseidon} respects each pod's requirements, but when
multiple multiple pods are viable it uses a global optimisation incorporating
utilisation metrics to choose the optimal placement. Koordinator
\cite{netto_koordinator_2018} similarly uses class-based static partitioning
(dedicating some portion of resources or priorities to LC vs. best-effort pods)
and then uses live node feedback to adjust placements and even migrate
workloads at runtime. By blending static and dynamic data, hybrid scheduling
strategies can enforce high-level policies (like fairness, or priority), and
ensure those policies aren't undermined by unforeseen runtime conditions. The
outcome is often superior QoS in multiple dimensions, but come at the cost of
the most complexity.

\subsection{Impact on QoS (Latency, Throughput, Fairness)}
The choice of data inputs in scheduling directly impacts various QoS metrics:

\textbf{Latency and Response Time:} Schedulers that can account for real-time
load significantly reduce latency for individual requests. A telemetry-aware
scheduler will avoid placing a latency-critical pod onto a node with high CPU or
IO wait, thereby ensuring that the pod can get CPU time quickly. In contrast, a
purely static scheduler might inadvertently co-locate many heavy pods on the same
node (if their requests were low but actual usage is high), leading to
contention and higher latency for all pods there. Thus telemetry-driven or
hybrid scheduling is better for meeting low-latency SLAs.

\textbf{Throughput and Completion Time:} For batch workloads, scheduling
strategies affects job completion times and overall throughput of the cluster.
Volcano's \cite{volcano_introduction} gang scheduling ensures all tasks of a
parallel job start together so the job can finish as quickly as possible with
no stragglers waiting for resources. This improves throughput of a batch
workload and avoids resource waste. Similarly, a scheduler using telemetry can
balance load by preventing certain nodes from becoming bottlenecks. However,
using utilisation metrics does not always guarantee optimal throughput. For
example, when 5 pods are running on a node, the node may advertise 100\% CPU
usage. When running an additional pod, CPU utilisation will remain at 100\% but
the resulting pod completion times may increase sub-linearly with the number of
pods and thus result in higher-throughput. This example highlights how
utilisation metrics can be misleading when detecting pod contention.

\textbf{Fairness and Resource Isolation:} Fairness can be considered from
multi-tenant perspectives (each user gets a fair share) and from a workload
isolation perspective (no noisy neighbour dominates a node to the detriment of
others). Pod spec schedulers address the former via quotas, priority classes,
etc., and the latter only indirectly (e,g, by requiring all pods to declare
resources, which if done correctly prevents one pod from consuming what another
needs). Telemetry-based approaches tackle isolation by detecting noisy-neighbour
effects - for instance, Koordinator monitors interference metrics and will
separate batch jobs from latency-senesitive ones if the interference crosses a
threshold.

\textbf{Resource Utilisation:} Spec-only scheduling often relies on
over-requesting (adding safety margins to resource requests) which can leave
nodes underused. Therefore, these schedulers might not be able to
completely use all of a node's capacity because they can't safely pack workloads
beyond the declared request. If a user under-requests, it can lead to CPU
throttling and OOM kills and degrade application QoS. On the other hand,
telemetry-aware scheduler can confidently increase utilisation up to a safe
limits - for example, Trimaran's TargetLoadPacking will keep filling a node
until it observes the node is about to hit a predefined utilisation limit.

DO I NEED TO INCLUDE CITATIONS FOR SYSTEMS I HAVE ALREADY REFERENCED

% \section{kube-scheduler (Default Kubernetes Scheduler)}
% \begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm] \textbf{Default Kubernetes
% Scheduler:} {\em Kube-scheduler selects an optimal node to run newly created or
% not yet scheduled pods. The description of containers in pods - and pods
% themselves - contain a set of predetermined requirements which the scheduler
% uses to influence its decisions. This makes the Kube-scheduler constraint-based
% (bin-packing). The Kube-scheduler selects a node for the pod in a 2-step
% operation: Filtering and Scoring. The filtering step finds the set of Nodes
% where its feasible to schedule the Pod, after which scoring is performed on the
% remaining nodes to identify the most suitable Pod placement. The highest ranking
% Node is chosen, with tiebreaks choosing at random.
%
% Having access to pod requirements simplifies scheduling logic to bin-packing,
% and allows the kube-scheduler to use simple greedy algorithms: filtering and
% scoring. However, knowing exact pod requirements is often not feasible,
% especially when pods behaviour can change during runtime. Kubernetes uses two
% sets of values when declaring pod resource usage: \verb|resources.requests| and
% \verb|resources.limits|. The values in \verb|resources.requests| are used in
% scheduling decisions and determine the amount of capacity a pod reserves on a
% node when it is bound to it. If a node has enough resources available, it is
% possible for a container to use more resources than its request specified.
% However, if specified, \verb|resource.limits| are applied by the kubelet and
% enforced by the kernel, throttling the container if it a container or pod
% exceedds its limits.
%
% Identifying the optimal values for the requests and limits is difficult and can
% result in resource underutilisation or resource starvation.
%
% TODO: SHOULD I INCLUDE A FIGURE WITH EXAMPLE WORKLOADS WHERE THIS MAY OCCUR?
% }  \end{tcolorbox}
%
% \section{Stand-alone Kubernetes Schedulers}
% \begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm] \textbf{Stand-alone
% Kubernetes Schedulers:} {\em In this chapter I will explore schedulers than have
% their own binary and do not extend the kubernetes scheduler. I will try to
% classify these schedulers into different classes and include strengths and
% weaknesses. I will focus on how each scheduler handles resource contention and
% QoS.}  \end{tcolorbox}
%
%
% \subsection{Framework Schedulers}
% \begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm] \textbf{Framework
% Schedulers:} {\em In this chapter I will explore schedulers that use the
% Kubernetes Framework to extend its behaviour. I will try to classify these
% schedulers into different classes and include strengths and weaknesses. I will
% focus on how each scheduler handles resource contention and QoS.}
% \end{tcolorbox}
%

\subsection{Summary of Related Work}
The earlier survey compared how existing Kubernetes schedulers used two
difference classes of input data - static pod information and telemetry - to
guide their scheduling decisions. Pod description-based scheduling ensures that
fundamental resource guarantees are respected. Telemetry-based scheduling can
more precisely allocate pods to nodes at the cost of increased complexity.
Hybrid schedulers combine these methods to provide guarantees of fundamental
resource requests while fine tuning scheduling decisions. However, their
reliance on predefined pod requirements still limits the QoS they can achieve.
Without careful consideration and engineering, erroneous pod resource requests
can result in under-utilisation or high resource contention.


\section{Serverless Workloads and Schedulers}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm] \textbf{Serverless
    Workloads:} {\em This section I will introduce the concept of serverless
    functions and how it is growing industry: adapted in multiple cloud
    services. Explain the typical workloads: resources, duration and actions.
Explain why containers are perfect for these workloads due to their fast start
up time compared to booting up VMs.

In this section I will also review existing
    Serverless schedulers. I will consider what type of scheduling information
    these schedulers will use and whether these schedulers consider very short
    workloads such as less than 4 minutes
}  \end{tcolorbox}

\section{Shortcomings of Common Telemetric Data}

Many telemetry-aware Kubernetes schedulers \cite{trimaran,
netto_koordinator_2018, gog_firmament_2016, poseidon} use utilisation
statistics, such as \% CPU used, \% memory used, as these metrics are available
in most computer systems and can be efficiently collected. While they are easy
to process, high values do not always signify over-contention. During the
implementation of the Spazio prototype, I investigate how CPU Utilisation
corresponds to Job Completion time. From the investigation, figure
\ref{fig:podcount-util-pressure} shows that throughput continues to increase as
we increase the number of concurrently running pods even after CPU utilisation
reaches 100\%. Therefore I conclude that CPU utilisations is not representative
of the true CPU capacity. \textbf{I only investigated this on my cluster of
VMs. This may just be a property of VM machines}.

Few Kubernetes schedulers use deeper contention indicators (e.g. cache pressure,
memory pressure and CPU throttling) \cite{intel-tas, netto_koordinator_2018}. I believe
that metrics can be used to better inform scheduling decisions compared to the
standard utilisation metrics.

\section{Pronto}
Pronto is a federated asynchronous, memory-limited algorithm proposed for
online task-scheduling across large-scale networks of hundreds of workers. Each
individual node updates a local model based on telemetric data and generates a
rejection signal which reflects the overall node responsiveness and whether it
can accept an incoming task. In addition, aggregation is performed on the local
models to build a global view of the system. Pronto takes the existing CPU Ready
metric generated by the VMware vSphere and proposes a novel method that uses it
as a task scheduling predictor. I believe that Pronto's use of contention
indicators and federation of individual node models can be used to implement a
telemetric-based QoS scheduler.

% \begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm] \textbf{Pronto:} {\em In
% this section I will give a brief overview of Pronto. I will mention how it is an
% online system that uses real-time data to build local model on individual nodes.
% I will also include that due to the nature of the models, local models can be
% aggregated into a more holistic global model, allowing for faster node
% bootstrappng and making more informed decisions.} \end{tcolorbox}

\subsection{Overview}
\label{pronto-overview}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/pronto}
    \caption{Projection of incoming $y \in \mathbb{R}^d$ onto embedding $U \in
    \mathbb{R}^{d \times r}$ producing $R$ projections in $v \in \mathbb{R}^{1
    \times r}$. Projections are tracked over time for detecting spikes which
    form the basis of the rejection signal. The sliding window for spike
    detection for each projection is of size $w$ also shown in the figure.}
    \label{pronto-components}
\end{figure}

PCA is a powerful dimensionality reduction technique, used to simplify
complex, high-dimensional datasets while retaining the most information. Pronto
use Federated PCA (a privacy preserving distributed approach to PCA) to extract
features which represent the "directions" with greatest variance. Pronto can
then project telemetry data onto these vector directions to predict future
spikes which indicate resource contention.

Pronto makes use of the following property: The PCA of a matrix can be
calculated by mean centering the data, performing SVD on the resulting matrix to
give $U$, the directions of the principle components, and $\Sigma$ the variance
along these directions. As Pronto aims to be memory-limited, it can't store all
of the collected telemetry and must instead take a streaming approach. It
periodically measures the CPU Ready metric at a given frequency. To perform
FPCA, every new batch of $b$ samples is iteratively merged into the latest
subspace using Incremental-SVD. In addition, to perform global model
aggregation, SVD is performed on the concatenated bases of two local models.
Pronto can further reduce the required computation by using Low-Rank
approximations, reducing the dimensionality to $r$ where $r < m$. It uses the
assumption that the first $r$ eignenvalues contain the majority of the
information of the dataset.

Once a node has a local model, it can periodically generate a signal by
projecting the latest telemetry data onto the principle component subspace $U$
and identifies all the spikes in the projections. If the weighted sum of the
spikes exceeds a threshold, then a rejection signal is raised which indicates
that node is experiencing performance problems and a job should not be
scheduled.

\subsection{Evaluation}

In the Pronto paper, a scheduler's effectiveness was quantified by its ability
to raise a rejection signal that precedes a CPU-Ready spike within a pre-defined
window, while minimising the number of false positives that do not precede
CPU-Ready spikes. Protono was compared against non-distributed
dimensionality-reduction methods \cite{}, and was shown to predict CPU-Ready
signals effectively while keeping false-positives low. Pronto's better
performance over non-distributed strategies implies that there were correlations
between job's resource usages on different nodes at the same point in time. As a
result, FPCA could use telemetry across multiple nodes to more acurrately
compute the principle components and thus give better CPU-Ready predictions.

These properties make Pronto an attractive technique that could be incorporated
into scheduling within a Kubernetes cluster.

\subsection{Limitations}

\subsubsection{Assumptions}
The Pronto paper only provides a method with which to measure node
"responsiveness" to future workloads. However, it does not include an allocation
algorithm which then uses this scoring. Furthermore, a binary signal makes it
difficult to score nodes against each other. Other schedulers typically use a
scoring function to pick the "optimal" node \cite{kube-scheduler}\cite{}.

In addition, the paper makes multiple assumptions which don't hold in a Kubernetes
scheduler. Firstly, it assumes there is no communication latency within the system.
This also implicitly assumes no binding latency: no latency between a node
accepting a task and the task starting to run on the node. This is important as
the spike prediction uses live telemetry data to predict spikes, and thus the
score only considers currently running task. In Kubernetes, the latency between
a pod being bound to a node and the pod actually running on the node is
significant \cite{tyczynski_kubernetes_2015}. This introduces another challenge to directly applying
Pronto to Kubernetes, as nodes may accept too many pods as the signal does not
reflect the "in flight" pods.

% The Pronto paper implements a binary "responsiveness" signal which predicts
% upcoming performance degradation. Because the authors assume a system with no
% communication latency (implicitly assuming that scheduled workloads were
% immediately visisble in the signal as well), they could send this signal
% directly to a central scheduler which could then stop assigning workloads once a
% node sent a Reject Signal.
%
% However, due to significant pod startup latency, the method can't be used in a
% real-world Kubernetes cluster is infeasible. When measuring pod startup in a
% 100 node clusters, the more than 50\% of pods took more than $\approx$ 1 second
% to startup. In addition, when nodes were 100\% full, pod startup could reach up
% to 4 seconds. This latency is significant when Kubernetes schedulers can
% support a throughput of $\approx$1000 pods per second
% \cite{qadeer_scaling_2022}. Applying the same approach as used in the paper,
% could result in nodes advertising a "willingness" to take on new pods while
% a large number of pods are in "flight" and once running will immediately
% overload the node. To prevent this runaway train type problem, I need to define
% a reservation function: a function that reserves an amount of the signal for a
% bound pod. This is necessary to allow previous scheduling decisions to have an
% imnpact on the signal while the signal updates to take into account the
% scheduled pods.
%
% In addition, telemetry-based schedulers can use individual node performance
% information to score and fine-tune pod allocations. A binary signal does not
% provide the necessary information for scoring nodes, potentially resulting in
% worse pod allocations.
%
% In summary, the requirements of the signal are:
% \begin{itemize}
    % \item Reservable: the scheduler must be able to track the pending impact of
        % previous scheduling decisions until the pods have begun running.
    % \item Comparable: the signal must provide enough information to score nodes
% \end{itemize}

\subsubsection{Peak-Prediction}
While CPU-Ready is specific to VMware vSphere, other contention metrics
are available on a Linux-based system. The closest related metrics are the
pressure stall information (PSI) metrics: pressure information for each resource
is exported through their respective file in
\verb$/proc/pressure/<cpu|memory|io>$. Within each file, the metrics are
broken down into two categories: some - which indicates the share of time in
which at least some tasks are stalled on a given resource, and full which
indicates the share of time in which all non-idle tasks are stalled on a given
resource simultaneously.

Pronto performs peak-detection on the metrics to predict future spikes which
then indicate high-resource contention and degraded performance. Therefore, for
Pronto to make accurate predictions and produce a correct RejectJob signal, PSI
must exhibit spikes during high-resource contention. To investigate the
feasibility of peak prediction within a Kubernetes node using PSI metrics, I
polled the \verb|/proc/pressure| files of nodes when under different workloads.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{images/pressure-baseline.png}
    \includegraphics[width=0.48\textwidth]{images/pressure-single.png} \\
    \includegraphics[width=0.48\textwidth]{images/pressure-smallbatch.png}
    \includegraphics[width=0.48\textwidth]{images/pressure-bigbatch.png}
    \caption{Measurements of \texttt{/proc/pressure/} \texttt{total} value under
    different loads. The container runtime results in spikes no matter the
    workload.}
    \label{fig:pressure}
\end{figure}

From figure \ref{fig:pressure}, we can see how even with lightweight workloads,
the PSI metrics experience a spike in value. These spikes are due to the
container runtime, in this case Containerd, using resource to create or delete
the containers of the pods.

The PSI metrics also expose an average over the last 10 seconds.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{images/avg-pressure-baseline.png}
    \includegraphics[width=0.48\textwidth]{images/avg-pressure-single.png} \\
    \includegraphics[width=0.48\textwidth]{images/avg-pressure-smallbatch.png}
    \includegraphics[width=0.48\textwidth]{images/avg-pressure-bigbatch.png}
    \caption{Measurements of \texttt{/proc/pressure/} \texttt{avg10} value
    under different loads. The container runtime results in spikes no matter the
    workload.} \label{fig:pressure-avg}
\end{figure}
Figure \ref{fig:pressure-avg} shows how the averaged value is able to reduce the
contention spikes. However, it also reduces the responsiveness of the metrics -
in the 10 pod batch, the metrics fails to rise to the same value as in the
total case before the pods have completed. From this investigation, I concluded
that peak-prediction with sub-second polling is not feasible within Kubernetes.

% In the paper, Pronto uses \verb|CPU-Reeady| which is generated by the VMware
% vSphere virtualisation platform. This metric can't be used within a
% Kubernetes cluster as machines can be both virtual and physical. Since Linux
% 4.20+, the kernel can track how long tasks are stalled waiting for the CPU
% at a cgroup granularity. By inspecting the the root cgroupâ€™s CPU pressure
% file using \verb|cat /proc/pressure/cpu| you can measure the total time all
% processes spent waiting for the CPU to be available.
%
% While this type of metric can be used to alert of performance degradation,
% this metric has a few shortcomings. Firstly, it only reports CPU-centric
% information. This is not always representative measure of resource
% contention as memory-heavy workloads may starve for RAM resources while
% metrics like \verb|CPU-Ready| and \verb|/proc/pressure/cpu| remain
% unaffected.
%
% Secondly, a significant amount of resources are used starting up or deleting
% containers. This results in large spikes, as shown in figure
% \ref{pressure-eval}, which are difficult to distinguish from genuine CPU-Ready
% spikes. As Pronto uses spike detection to predict future resource performance
% degradation, container start-ups could produce detectable spikes which would
% reduce the rate at which pods are assigned to nodes and could result in lower
% throughput.
%

\section{Summary}

This chapter introduced the core concepts of online scheduling and the
Kubernetes environment. I explored existing Kubernetes schedulers, investigating
the merits of using pod description-based schedulers vs. telemetry-based
schedulers. From the brief survey, I identified that few Kubernetes schedulers
use deeper contention indicators, such as cache pressure and memory pressure.
A potential technique that could be applied to Kubernetes scheduling is Pronto:
a novel scoring method that uses a federated approach with contention-based
metric - allowing it to quickly predict performance degredation while catering
for distributional shifts. While Pronto can't be directly applied to Kubernetes,
its federated apsect and its use of contention-based metrics can inspire a new
FL approach.

% \begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    % \textbf{Summary:} {\em In this chapter I will summarise the problem and
    % problem space. I will review the findings of the related work, highlighting
    % weaknesses of existing Kubernetes schedulesr with respect to QoS
    % scheduling.}
% \end{tcolorbox}

