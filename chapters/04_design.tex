\chapter{Design and implementation}
% This chapter may be called something else\ldots but in general the
% idea is that you have one (or a few) ``meat'' chapters which describe
% the work you did in technical detail.
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Design and Implementation:} {\em In this section, I will outline the
    goals of my system. I will give a brief overview of the chapture structure,
    summarising each core section and what I achieve.}
\end{tcolorbox}

\section{Applying Pronto to Containers}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Applying Pronto to Containers:} {\em In this section I will outline
    the reasons for why I made modifications to the Pronto system.}
\end{tcolorbox}

\subsection{CPU-Ready Metric}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{CPU Ready Metric:} {\em I will talk about the problems
    of using CPU Ready or a CPU-Ready-like metric. Pronto uses \verb|CPU Ready|
    which is only available to VMs, and thus not applicable to a Kubernetes
    clusters where nodes are possibly also physical and virtual machines. Since
    Linux 4.20+, the kernel can track how long tasks are stalled waiting for the
    CPU at a cgroup granularity. By inspecting the the root cgroupâ€™s CPU
    pressure file using \verb|cat /proc/pressure/cpu| you can measure the total
    time all processes spent waiting for the CPU to be available.

    While this type of metric can be used to alert of performance degradation,
    this metric has a few shortcomings. Firstly, it only reports CPU-centric
    information. This is not always representative measure of resource
    contention as memory-heavy workloads may starve for RAM resources while
    metrics like \verb|CPU-Ready| and \verb|/proc/pressure/cpu| remain
    unaffected.

    Secondly, a significant amount of resources are used starting up or deleting
    containers. This results in large spikes in $y$ (observed data point) which
    are difficult to distinguish from genuine CPU-Ready spikes and can result in
    false rejections. This rejection on start-up would reduce the rate at which
    pods are assigned to nodes and could result in lower throughput.

    I decided to adapt Pronto to use fine-grained time-series measurements such
    as the CPU utilisation, memory consumption to provide real-time performance
    data of individual nodes as proposed in \cite{grammenos_pronto_2021}.
    }
\end{tcolorbox}

\subsection{Rejection Signal}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Rejection Signal:} {\em In this section I will discuess the possible
    drawbacks to using a binary rejection signal. Pronto proposes a means
    of predicting resource contention but does not include a means for
    allocating based on the Rejection-Signal.

    The Pronto paper assumes a system with no communication latency, which also
    implicitly includes no pod start-up latency. This simplifies
    scheduling decisions as the effects of prior decisions are immediately
    visible in the next signal update (typically 100ms-1s later). However, in
    Kubernetes, pod start-up can take from 1s-10s. If trying to schedule pods
    with a high throughput (the Kubernetes scheduler can support a throughput of
    $\approx$1000 pods per second \cite{qadeer_scaling_2022}), this can result in
    hundreds of pods waiting to start-up on a node while it still advertises an
    Accept-Signal.

    In addition, a binary signal is difficult to use when trying to score nodes
    and determine the optimal Pod placement. Therefore, I decided to extend
    Pronto's output to a continous signal in $\Re$. By allowing nodes to
    advertise their responsiveness along a finite scale, more fine-grained
    comparisons can be made between nodes.}
\end{tcolorbox}

\subsection{Name TBD}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Name TBD:} {\em In this section I outline a new Pronto-based system.
    I summarise the core changes: different metrics, new subspace merging,
    new signal function. }
\end{tcolorbox}

\subsection{Resource Usage Subspace}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Resource Usage Subspace:} {\em In this section I outline the new
    vector space over which indivdual nodes build their local model. I decided
    to collect CPU utilisation and memory usage as these metrics are easily
    accessible and are used in a variety of industry-standard schedulers
    \cite{hadoop2016apache,sahasrabudhe_improved_2015}. These metrics give the
    proportion of a resource currently being used, ranging from [0,1].

    Metric Server is a cluster-level add-on that provides near real-time CPU and
    memory usage metircs for Pods and Nodes. These metrics are easily accessed through
    the \verb|metrics.k8s.io/v1| APIService and are updated by the Metric Server scraper
    periodically (default every 15s). This method of metric collection is not
    suitable. Certain Pods may complete in less than 15 seconds and thus may not
    be detected by the signal. In addition, it would take $15 \times \text{batch size}$
    seconds between model updates (required to collect a single
    batch before performing subspace merging), and would result in a less
    representative and out-of-date model of "current" resource usage.

    Instead, I decided to have a pod running on each node, scraping
    metrics from files within the Linux \verb|/proc| directory.
    \verb|/proc/stat| reports the cumulative count of "jiffies" (typically
    hundredths of a second) each cpu spent in a specific mode \cite{proc_stat5}.
    I can then calculate cpu utilisation using:
    \[ \text{CPU Usage\%} = 1 - \frac{\Delta(\text{idle} +
    \text{iowait}}{\Delta{\Sigma \text{all fields}}} \]
    \verb|/proc/meminfo| shows a snapshot of memory usage in kilobytes. The
    percentage of memory used can then be calculated from the provided fields:
    \[ \text{Memory Used\%} = 1 - \frac{\text{MemFree} +
    \text{Buffers} + \text{Cached}}{\text{MemTotal}}\]
    Due to the higher refresh rate, we can poll these files more frequently
    ($\approx$ 10Hz), and can remove the network latency that would come from
    making calls to the Metric Server APIService.
    }
\end{tcolorbox}

\subsection{Subspace Merge}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Resource Usage Subspace:} {\em In this section I outline the new
    subspace merge operations. These metrics give the proportion of a resource
    currently being used, and thus the original Pronto subspace operations have
    different meanings. Performing the original mean-centering as part of FPCA
    would result in the following matrices:
    \begin{itemize}
    \item $U$ - defines the principle components of the variance within the
    collected telemetry performance data.
    \item $\Sigma$ - represents the variance along those principle components.
    \end{itemize}
    While this could be used to detect spikes in resource usage, positive spikes
    do not translate to possible resource-contetion.
    TODO: Include a diagram of how this signal would falsely predict contention
    when spiking from 30\% to 70\% usage but falsely predict no contention when
    a resource is at 100\% usage.

    Instead we can perform Incremental-SVD directly on the raw [0,1] metrics.
    This results in the following matrices:
    \begin{itemize}
    \item $U$ - The principle components of the resource usage experienced by
    the node.
    \item $\Sigma$ - The sum of the projected distances of all the samples
    measured in the directions given by $U$
    \end{itemize}
    TODO: Include proofs in the appendices
    The column eigenvectors of $U$ and the eigenvalues of $\Sigma$ predict the
    resource usage characteristics of the pods running on a node. This was
    achieved without prior knowledge of pod resource usage.

    As we are no longer mean-normalising the matrices before performing SVD, the
    resulting $\Sigma$ grows with every iterative-SVD.
    TODO: Include proof in the appendices.
    TODO: Give example of Sigma exploding
    To overcome this issues, scale both input matrices by $1/\sqrt{2}$. The
    resulting matrix from this Subspace Merge behaves similar to an EMA with a
    forgetting factor of 0.5.

    TODO: Investigate whether the modified aggregation function is correct.
}
\end{tcolorbox}

\subsection{Pronto Signal}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Pronto Signal:} {\em In this section I outline the new Signal
    function:
    \[ y_{\text{expected}} = y + k \sum_i \frac{\sigma_i u_i}{\text{Tr}(\Sigma)} \\
        max_k. \forall i: y_{\text{expected}} < 1 \]

    Explain how the weighted sum of the eigenvectors gives the expeected
    increase in resource usage. As the signal has no concept of the total number
    of pods, $k$ instead gives a measurement of the available capacity that a
    node predicts a future pods could use.

    TODO: Include a diagram or example showing how given a high usage in CPU, if
    a model predicts pods are memory centric then it will still give a high
    signal value.
    }
\end{tcolorbox}

\subsection{Denoising Signal}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Denoising Signal:} {\em In this section, I introduce the problem
    that comes with measure resource usage directly.
    TODO: Include a diagram showing Container Events and how the signal
    fluctuates due to higher CPU usage. In addition, include the dampened signal
    Mention how this noise could slow down scheduling decisions, cause phantom
    spikes in the signal after which the resource usage dropped once pods were
    running. Point out how we want to measure the resource usage of pods and not
    the container environment which lasts less than a second. In addition,
    mention how this would have made it difficult later on to determine the
    impact of a pod on a signal and thus its reserve amount.

    Explain how I used a Dynamic EMA filter. The filter starts of with a slow
    exponential ($\approx$ 0.01), but after more than 4 high samples it switches
    to a faster exponential ($\approx$ 0.25). This number of samples was decided
    from an investigation into Container Events (resource usage spikes from
    container creation or deleting typically lasted less than 100ms).

    TODO: Could do a more thorough investigation with a table containing event
    time distribution

    Explain that applying the filter to the direct collected metrics, rather
    than the signal was better to reduce the noise being introduced into a
    node's local model.
    }
\end{tcolorbox}

\section{System Design}
\subsection{Overview}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Overview:} {\em In this section, I give a brief overview of the
    Pronto system. I outline the individual remote schedulers which reside on
    each node and collect resource usage metrics and generate the acceptance
    signal.

    In addition, outline the Aggregation Service, with a central server and the
    remote schedulers which act as clients. The remote schedulers send their
    latest local model and the aggregate server responds with the latest
    aggregate model. To reduce latency, the aggregate model services each
    aggregation request by first enqueuing the local model to be aggregated and
    immediately sending a response with the latest version of the global model.
    This system trades consistency for latency, which is important when scaling
    to hundreds of nodes.

    TODO: Investigate the latency of throughput/limit of the aggregation server.

    Finally, give a brief overview of the central scheduler. Explain how it is a
    Framework Scheduler and thus replaces the Filter and Score functions}
\end{tcolorbox}

\subsection{Framework Scheduler}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Framework Scheduler:} {\em In this section, I explain design of the central
    scheduler. I will give the reasons for why I decided to use a Framework
    Scheduler: use of efficient data structures, extensibility allows me to
    encode the new Filter and Score behaviour.

    Briefly explain the Filter, Score and Reserve semantics.
    }
\end{tcolorbox}
\subsection{Reserve Function}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Framework Scheduler:} {\em In this section, I explain how I
    calculated the reserve quantity. I needed a method to estimate the amount of
    the signal to reserve when assigning a pod such that following scheduling
    decisions can see the future impact of the pod without having to wait for
    the signal to update / wait for the pod to start running.

    Explain how I used several methods:
    \begin{itemize}
        \item Fixed cost determined before initialisation. Does not adapt to
            different workloads.
        \item Kalman Filter to predict the change in signal given the number of
            pods added. Difficult to perform when multiple pods are being added
            and deleted at a time. Hard to extract the actual impact of a pod.
        \item 2D Kalman Filter to predict the signal based on the underlying
            equation: $\text{signal} = \text{capacity} + \text{per pod cost} \times \text{no. of pods}$.
            This method was relatively accurate, but
            would result in oscillations as the learned covariance between
            capcity and per-pod-cost resulted in fluctuations. In addition,
            trying to increase reactivity to changes in the signal, resulted in
            this fluctuations becoming extreme.
        \item 2 separate 1D Kalman Filters, one predicting the per-pod-cost
            while the other predicts the capacity. This method worked the best.
            By separating the variables, I was able to prevent the oscillations
            in the previous method.
    \end{itemize}
    }
\end{tcolorbox}
\subsection{Optimisations}
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Optimisations:} {\em In this section I explain the optimisation I
    implemented to improve scheduler throughput. I will include a figure of node
    utilisation when using the standard system, showing how extra capacity
    remains do to error between the true per-pod-cost and the estimated value.

    I also show the relation of pod-completion time to number of pods running on
    a node, alongside the signal. I will argue there are downsides to using CPU
    Utilisation. That once a CPU is fully utilised (no "jiffies" are spent in the
    idle state), the addition of more pods can't increase the CPU Utilisation
    and thus the signal does not change. In addition, the relation between
    pod-completion time vs pod count is typically sub-linear and thus encourages
    packing more pods on a node, even when the signal identifies full resource
    utilisation.

    An optimisation I proposed was to borrow the concept from TCP flow-control.
    This method uses a moving window of the latest n pod completion times, which
    it uses to identify trends or potential spikes. The challenge in this
    situation is that the method needs to be as efficient as possible while also
    being able to handle pods of different workloads and completion times
    without prior knowledge of different classes.

    I compare the change in the sum of square completion times against a
    threshold. I use the sum of squares to penalize more an increase in the
    completion of longer running jobs. This is because a proportional increase
    in completion time has a worse impact the longer the job runs for. Whenever
    an incoming entry keeps the sum below the threshold, the overprovision
    capacity increases, while any completion times that exceed the threshold
    result in the overprovision capacity to halve. This results in a similar
    saw-tooth behaviour seen in TCP and allows nodes to accept more pods while
    keeping.
    }
\end{tcolorbox}


